{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantix\n",
    "## Big Data Engineer\n",
    "### Projeto final de Spark - nível básico\n",
    "\n",
    "Campanha Nacional de Vacinação contra Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabela de conceitos** (retirado do site https://www.coronavirus.sc.gov.br/sobre-os-dados/)  \n",
    "\n",
    "- Casos confirmados: Cada caso confirmado é um paciente que teve a infecção por coronavírus confirmada por exame laboratorial (teste rápido sorológico ou RT-PCR) ou por vínculo epidemiológico. Os gráficos que exibem a evolução deste número ao longo do tempo levam em consideração a data de início dos sintomas, permitindo identificar com maior precisão o momento da infecção pelo vírus SARS-CoV-2. Quando a data de início dos sintomas não está disponível, a referência utilizada é a data de coleta do exame. Os casos que evoluem a óbito e os recuperados estão incluídos neste número.\n",
    "\n",
    "- Óbitos: Pacientes que faleceram em decorrência da Covid-19, com confirmação laboratorial ou por critério clínico epidemiológico. Os gráficos que exibem a evolução deste número ao longo do tempo levam em consideração a data do óbito.\n",
    "\n",
    "- Recuperados: Estimativa realizada com base no tempo decorrido a partir do início dos sintomas e a evolução de cada caso. Entram no cálculo os pacientes que tiveram início de sintomas há pelo menos 14 dias, não evoluíram a óbito e não se encontram em internação hospitalar. Uma limitação a ser considerada nessa estimativa é o fato de ser possível a existência de pacientes que, mesmo se enquadrando nos critérios, ainda estejam em acompanhamento, assim como há a possibilidade de se atingir a recuperação antes do período de 14 dias.\n",
    "\n",
    "- Letalidade: Taxa percentual de casos confirmados que evoluíram a óbito. Método de cálculo: (número de óbitos x 100) / número de casos confirmados.\n",
    "\n",
    "- Incidência: Quantidade proporcional de casos confirmados para cada 1 milhão de habitantes. A população considerada no cálculo é a estimativa populacional para 2019 do Instituto Brasileiro de Geografia e Estatística (IBGE), salvo quando especificada referência distinta. Método de cálculo: (casos confirmados * 1.000.000) / população.\n",
    "\n",
    "- Mortalidade: Quantidade proporcional de óbitos para cada 1 milhão habitantes, seguindo a mesma metodologia de cálculo da incidência de casos confirmados. Método de cálculo: (óbitos * 1.000.000) / população."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Enviar os dados para o hdfs\n",
    "\n",
    "Resolução:\n",
    "\n",
    "1. abri um terminal linux\n",
    "2. acessei o diretório com o comando\n",
    "```\n",
    "cd treinamentos/spark\n",
    "```\n",
    "3. baixei os dados sobre a campanha de vacinação do covid-19 com o comando\n",
    "```\n",
    "sudo curl -O https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "```\n",
    "4. instalei o programa para fazer a extração dos dados que estavam compactados com o comando\n",
    "```\n",
    "sudo apt install unrar\n",
    "```\n",
    "5. extraí os dados com o comando\n",
    "```\n",
    "unrar x 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "```\n",
    "6. movi os arquivos extraídos para o diretório /input com o comando\n",
    "```\n",
    "sudo mv *.csv /home/pmoraes/treinamentos/spark/input\n",
    "```\n",
    "7. inicializei o docker com o comando \n",
    "```\n",
    "docker-compose start\n",
    "```\n",
    "8. acessei o container namenode com o comando\n",
    "```\n",
    "docker exec -it namenode bash\n",
    "```\n",
    "9. criei uma estrutura de diretórios para receber os arquivos do linux com o comando\n",
    "```\n",
    "hdfs dfs -mkdir -p /user/paula/spark/projeto_final\n",
    "```\n",
    "10. transferi os arquivos do linux para o hadoop com o comando\n",
    "```\n",
    "hdfs dfs -put /input/*.csv /user/paula/spark/projeto_final\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2 - Otimizar todos os dados do HDFS para uma tabela Hive particionada por municipio\n",
    "\n",
    "Resolução:\n",
    "1. importei as bibliotecas e criei a sessão do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName('Projeto final de Spark - Campanha Nacional de Vacinação contra Covid-19')\\\n",
    ".config('spark.some.config.option', 'some-value')\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. criei o dataframe através da leitura dos arquivos CSV no diretório do hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('hdfs://namenode/user/paula/spark/projeto_final/', sep=\";\",header=True, inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. imprimi o schema do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. transformei o campo data no formato de tempo unix e visualizei de forma vertical os 6 primeiros registros, de modo a se ter uma melhor visualização da porção do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-25 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 0          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "-RECORD 1----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-26 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 1          \n",
      " casosNovos             | 1          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "-RECORD 2----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-27 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 1          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "-RECORD 3----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-28 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 1          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "-RECORD 4----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-29 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 2          \n",
      " casosNovos             | 1          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "-RECORD 5----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-03-01 \n",
      " semanaEpi              | 10         \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 2          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_to_unix = df.withColumn('data', from_unixtime(unix_timestamp(df.data), 'yyyy-MM-dd'))\n",
    "df_to_unix.show(6, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. criei a base da dados chamada covid e salvei o dataframe no hdfs particionado pelo municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database covid\")\n",
    "df_to_unix.write.mode('overwrite').partitionBy('municipio').format('csv').saveAsTable('covid.municipio', path='hdfs://namenode:8020/user/hive/warehouse/covid/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. executei o comando para, no diretório do hdfs, listar os arquivos mostrando os 10 primeiros e os 10 últimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5299 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/_SUCCESS\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abadia de Goiás\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abadia dos Dourados\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abadiânia\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abaetetuba\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abaeté\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abaiara\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abaré\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Abatiá\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Águas de São Pedro\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Águia Branca\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Álvares Florence\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Álvares Machado\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Álvaro de Carvalho\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Áurea\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Ângulo\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Érico Cardoso\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Óbidos\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 16:42 /user/hive/warehouse/covid/municipio=Óleo\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. criei uma cópia do dataframe para realizar os próximos exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df_to_unix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. visualisei as bases de dados e usando a base covid visualizei as tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|       covid|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|   covid|municipio|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use covid\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3 - Criar as 3 visualizações pelo Spark com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização 1 - Casos recuperados e Em acompanhamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|casos_recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Casos recuperados\n",
    "df_casos_recuperados = spark.sql(\"select Recuperadosnovos as casos_recuperados from municipio order by 1 desc limit 1\")\n",
    "df_casos_recuperados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|em_acompanhamento|\n",
      "+-----------------+\n",
      "|          1317658|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Em acompanhamento\n",
    "df_em_acompanhamento = spark.sql(\"select emAcompanhamentoNovos as em_acompanhamento from municipio order by 1 desc limit 1\")\n",
    "df_em_acompanhamento.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização 2 - Casos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Acumulado\n",
    "df_acumulado = spark.sql(\"select casosAcumulado as acumulado from municipio order by 1 desc limit 1\")\n",
    "df_acumulado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|casos_novos|\n",
      "+-----------+\n",
      "|     115228|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Casos novos\n",
    "df_casos_novos = spark.sql(\"select casosNovos as casos_novos from municipio order by 1 desc limit 1\")\n",
    "df_casos_novos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|incidencia|\n",
      "+----------+\n",
      "|    9999.8|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Incidência\n",
    "df_incidencia = spark.sql(\"select cast(((casosAcumulado*100000)/populacaoTCU2019) as decimal(5,1)) as incidencia from municipio order by 1 desc limit 1\")\n",
    "df_incidencia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização 3 - Óbitos confirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|obitos_acumulados|\n",
      "+-----------------+\n",
      "|           526892|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Óbitos acumulados\n",
    "df_obitos_acumulados = spark.sql(\"select obitosAcumulado as obitos_acumulados from municipio order by 1 desc limit 1\")\n",
    "df_obitos_acumulados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|obitos_novos|\n",
      "+------------+\n",
      "|        4249|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Casos de óbitos novos \n",
    "df_obitos_novos = spark.sql(\"select obitosNovos as obitos_novos from municipio order by 1 desc limit 1\")\n",
    "df_obitos_novos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|letalidade|\n",
      "+----------+\n",
      "|    1500.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Letalidade\n",
    "df_letalidade = spark.sql(\"select cast(((obitosAcumulado * 100) / casosAcumulado) as decimal(5,1)) as letalidade from municipio order by 1 desc limit 1\")\n",
    "df_letalidade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|mortalidade|\n",
      "+-----------+\n",
      "|      999.1|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Mortalidade\n",
    "df_mortalidade = spark.sql(\"select cast(((obitosAcumulado * 100000) / populacaoTCU2019) as decimal(4,1)) as mortalidade from municipio order by 1 desc limit 1\")\n",
    "df_mortalidade.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4 - Salvar a primeira visualização como tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casos_recuperados.write.format('csv').saveAsTable('recuperados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em_acompanhamento.write.format('csv').saveAsTable('acompanhamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. visualização das tabelas salvas na base de dados covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "|   covid|acompanhamento|      false|\n",
      "|   covid|     municipio|      false|\n",
      "|   covid|   recuperados|      false|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5 - Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acumulado.write.option('compression', 'snappy').parquet('hdfs://namenode/user/paula/spark/projeto_final/exercicios/casos_acumulados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casos_novos.write.option('compression', 'snappy').parquet('hdfs://namenode/user/paula/spark/projeto_final/exercicios/casos_novos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidencia.write.option('compression', 'snappy').parquet('hdfs://namenode/user/paula/spark/projeto_final/exercicios/incidencia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 18:07 /user/paula/spark/projeto_final/exercicios/casos_acumulados\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 18:08 /user/paula/spark/projeto_final/exercicios/casos_novos\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-06 18:09 /user/paula/spark/projeto_final/exercicios/incidencia\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/paula/spark/projeto_final/exercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 6 - Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obitos_acumulados.selectExpr(\"to_json(struct(*)) AS value\").write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'obitos_acumulados').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|value                       |\n",
      "+----------------------------+\n",
      "|{\"obitos_acumulados\":526892}|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topico_obitos_acumulados = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe','obitos_acumulados').load()\n",
    "\n",
    "conteudo_obitos_acumulados = topico_obitos_acumulados.select(col('value').cast('string'))\n",
    "conteudo_obitos_acumulados.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obitos_novos.selectExpr(\"to_json(struct(*)) AS value\").write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'obitos_novos').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|value                |\n",
      "+---------------------+\n",
      "|{\"obitos_novos\":4249}|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topico_obitos_novos = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe','obitos_novos').load()\n",
    "\n",
    "conteudo_obitos_novos = topico_obitos_novos.select(col('value').cast('string'))\n",
    "conteudo_obitos_novos.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_letalidade.selectExpr(\"to_json(struct(*)) AS value\").write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'letalidade').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|value                |\n",
      "+---------------------+\n",
      "|{\"letalidade\":1500.0}|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topico_letalidade = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe','letalidade').load()\n",
    "\n",
    "conteudo_letalidade = topico_letalidade.select(col('value').cast('string'))\n",
    "conteudo_letalidade.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortalidade.selectExpr(\"to_json(struct(*)) AS value\").write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'mortalidade').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|value                |\n",
      "+---------------------+\n",
      "|{\"mortalidade\":999.1}|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topico_mortalidade = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe','mortalidade').load()\n",
    "\n",
    "conteudo_mortalidade = topico_mortalidade.select(col('value').cast('string'))\n",
    "conteudo_mortalidade.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 7 - Criar a visualização pelo Spark com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "__df = _df.groupBy(['regiao', 'estado']).agg({'casosAcumulado':'max', 'obitosAcumulado':'max', 'populacaoTCU2019':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------------------+-------------------+--------------------+\n",
      "|  regiao|estado|max(populacaoTCU2019)|max(casosAcumulado)|max(obitosAcumulado)|\n",
      "+--------+------+---------------------+-------------------+--------------------+\n",
      "|   Norte|    TO|              1572866|             200243|                3266|\n",
      "|   Norte|    AC|               881935|              85997|                1760|\n",
      "|   Norte|    PA|              8602865|             557708|               15624|\n",
      "|Nordeste|    MA|              7075181|             322052|                9190|\n",
      "|     Sul|    RS|             11377239|            1235914|               31867|\n",
      "| Sudeste|    SP|             45919049|            3809222|              130389|\n",
      "|Nordeste|    PI|              3273227|             299084|                6662|\n",
      "|   Norte|    AP|               845731|             118066|                1857|\n",
      "| Sudeste|    MG|             21168791|            1836198|               47148|\n",
      "|     Sul|    PR|             11433957|            1308643|               31692|\n",
      "+--------+------+---------------------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ajuste_nome_dos_campos = __df.withColumnRenamed('max(populacaoTCU2019)','populacao').withColumnRenamed('max(casosAcumulado)', 'casos_acumulados').withColumnRenamed('max(obitosAcumulado)','obitos_acumulados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----------------+-----------------+\n",
      "|  regiao|estado|populacao|casos_acumulados|obitos_acumulados|\n",
      "+--------+------+---------+----------------+-----------------+\n",
      "|   Norte|    TO|  1572866|          200243|             3266|\n",
      "|   Norte|    AC|   881935|           85997|             1760|\n",
      "|   Norte|    PA|  8602865|          557708|            15624|\n",
      "|Nordeste|    MA|  7075181|          322052|             9190|\n",
      "|     Sul|    RS| 11377239|         1235914|            31867|\n",
      "| Sudeste|    SP| 45919049|         3809222|           130389|\n",
      "|Nordeste|    PI|  3273227|          299084|             6662|\n",
      "|   Norte|    AP|   845731|          118066|             1857|\n",
      "| Sudeste|    MG| 21168791|         1836198|            47148|\n",
      "|     Sul|    PR| 11433957|         1308643|            31692|\n",
      "+--------+------+---------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ajuste_nome_dos_campos.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inclusao_de_incidencia_e_mortalidade = (df_ajuste_nome_dos_campos.withColumn('incidencia', round(df_ajuste_nome_dos_campos['casos_acumulados']/df_ajuste_nome_dos_campos['populacao']*100000,1)).withColumn('mortalidade', round(df_ajuste_nome_dos_campos['obitos_acumulados']/df_ajuste_nome_dos_campos['populacao']*100000,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----------------+-----------------+----------+-----------+\n",
      "|  regiao|estado|populacao|casos_acumulados|obitos_acumulados|incidencia|mortalidade|\n",
      "+--------+------+---------+----------------+-----------------+----------+-----------+\n",
      "|   Norte|    TO|  1572866|          200243|             3266|   12731.1|      207.6|\n",
      "|   Norte|    AC|   881935|           85997|             1760|    9750.9|      199.6|\n",
      "|   Norte|    PA|  8602865|          557708|            15624|    6482.8|      181.6|\n",
      "|Nordeste|    MA|  7075181|          322052|             9190|    4551.9|      129.9|\n",
      "|     Sul|    RS| 11377239|         1235914|            31867|   10863.0|      280.1|\n",
      "| Sudeste|    SP| 45919049|         3809222|           130389|    8295.5|      284.0|\n",
      "|Nordeste|    PI|  3273227|          299084|             6662|    9137.3|      203.5|\n",
      "|   Norte|    AP|   845731|          118066|             1857|   13960.2|      219.6|\n",
      "| Sudeste|    MG| 21168791|         1836198|            47148|    8674.1|      222.7|\n",
      "|     Sul|    PR| 11433957|         1308643|            31692|   11445.2|      277.2|\n",
      "+--------+------+---------+----------------+-----------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inclusao_de_incidencia_e_mortalidade.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 8 - Salvar a visualização do exercício 6 em um tópico no Elastic\n",
    "\n",
    "Resolução\n",
    "\n",
    "1. criei uma cópia do dataframe e salvei no HDFS em formato csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_inclusao_de_incidencia_e_mortalidade\n",
    "df_final.write.format(\"csv\").save('hdfs://namenode/user/paula/spark/projeto_final/output/covid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. copiei o arquivo do HDFS para o linux com o comando\n",
    "```\n",
    "hdfs dfs -get /user/paula/spark/projeto_final/output/covid.csv /treinamentos/spark/data\n",
    "```\n",
    "3. copiei o arquivo salvo no linux para a minha máquina local (windows)\n",
    "4. acessei o diretório do elasticsearch e inicializei o container com o comando\n",
    "```\n",
    "docker-compose start\n",
    "```\n",
    "5. após inicializar os containers, acessei o elastic no meu navegador através do localhost:5601\n",
    "6. importei o csv através do caminho \n",
    "```\n",
    "home > upload files > import csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 9 - Criar um dashboard no Elastic para visualização dos novos dados enviados\n",
    "\n",
    "Resolução\n",
    "\n",
    "1. acessei o menu \"Visualize\" do Kibana e fiz alguns testes de visualizações, no entanto, encontrei dificuldades em criar visualizações interessantes, abaixo estão algumas que montei a partir dos testes:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d2b41a504a4fb6a861bb88b0cac5588e47bebf34d6ee6f9d55266ab4c36ac2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
